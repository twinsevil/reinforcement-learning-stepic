# Проект на основе онлайн курса по нейронным сетям с платформы Stepic.

Необходимо было реализовать модель машинного обучения с подкреплением. Задание выполнено с привлечением библиотеки [Pygame](http://www.pygame.org/download.shtml) для отрисовки графики. 

**Цель:** научить машинку проходить заданную трассу против часовой стрелки.

![](https://pp.userapi.com/c855024/v855024043/2072e/pOnqFbAjn28.jpg)

Машинка получает расстояния до стен впереди нее (в поле зрения), свою текущую скорость (скаляр) и то, под каким углом к направлению на центр она едет (если машина повёрнута в направлении ![\vec{a}](http://www.sciweavers.org/upload/Tex2Img_1555005884/render.png) и при этом вектор от его положения до центра поля — ![\vec{b}](http://www.sciweavers.org/upload/Tex2Img_1555006103/render.png), то знать она будет ![\sin(\vec{a}, \vec{b})](http://www.sciweavers.org/upload/Tex2Img_1555007178/render.png), при этом угол всегда измеряется против часовой стрелки). На всякий случай: этому синусу хорошо бы быть поближе к −1, ведь тогда машина будет ехать точно по окружности вокруг центра, против часовой стрелки!



Несколько раз в секунду (задаётся параметром timedelta) агент принимает решение о том, в каком направлении придать ускорение  (![pm {pi} / {2}](http://www.sciweavers.org/upload/Tex2Img_1555006853/render.png) или 0), и какой величины будет это ускорение: 0.75, 0, −0.75.

Полное описание задания можно найти по [ссылке](https://stepik.org/lesson/21780/step/1?unit=5198).

# Пояснения к коду

Файл **cars/world.py** содержит класс «мира» (**SimpleCarWorld**). В мире есть карта и агенты (машинки), + физика, которая сопоставляет текущему состоянию мира (учитывая решения агентов, их положение и т.д.) следующее. Мир «живёт» пошагово. У мира есть набор констант, которые определяют, какую награду или наказание получит агент на текущем шаге. Осознать эти константы критически важно. Можно их менять и добавлять свои. Посмотрите внимательно на метод reward.

Файл **cars/agent.py** -  содержит класс **SimpleCarAgent**, у которого особенно важны методы choose_action и receive_feedback. Когда приходит время действовать (мир вызывает у агента метод **choose_action**), агент получает на вход «vision», то есть своё видение мира, или «мир с точки зрения агента». Основной орган чувств агента — «ладар». Запустите скрипт и вам сразу станет понятно, что такое этот «ладар». Другие компоненты видения агента уже описаны в начале текущего степа.

Метод **receive_feedback** получает от мира поощрение или наказание. Агент старается учитывать, что награда или наказание от мира может приходить «не сразу», а с задержкой. Поэтому агент произвольным образом распределяет награду по истории своих действий. Изначально реализована «экспоненциально затухающая награда». В этом же методе мы доучиваем нашу нейросеть, чтобы учиться на своих ошибках.

Агентом управляет нейронная сеть, параметры нейросети задаются в методе __init__ класса агента, или определяются на основе файла весов (об этом ниже). Эти нейронные сети вы будете учить под разные задачи, после чего веса этих нейросетей будут сдаваться в соответствующих степах данного урока.

Файл **run_car.py** - позволяет запускать машинки по сгенерированной трассе, а точнее:

1) Обучать модель с нуля. Для этого нужно оставить строчку `filename = ""` без изменений. Когда вы внесёте необходимые константы в места, которые не "компилируются", и снова запустите **run_car.py** (из консоли или напрямую), начнётся обучение модели. Если вы закроете окошко Pygame, веса нейронной сети, управляющей машинкой, сохранятся в файл с соответствующим именем (на Windows окошко часто не закрывается, но веса всё-таки сохраняются).

2) Продолжить обучать модель. Указываем `filename = "my_model.txt"`, и веса вашей машинки будут загружены из файла с соответствующим именем.

3) Оценить модель. Меняем **evaluate** на True и **steps** на количество шагов, на которых будет производиться оценка, и вместо обучения происходит только оценка модели.

# Базовое решение

Мы решили свести имеющуюся задачу к давно знакомой нам задаче регрессии. Добавив в класс **Network** возможность менять активационную функцию нейронов выходного слоя, мы получили готовую к работе сеть. 

Каждое состояние машинки мы можем оценить, исходя из наших целей. Это и делает функция **reward(self, state, collision)**: принимая на вход наше текущее состояние и информацию о том, столкнулись ли мы со стеной, она выдаёт оценку этого состояния (критерии оценки вы должны задать сами!).

Идея нашего решения чрезвычайно проста. У нас есть входные данные (расстояния до границ трассы, модуль скорости, синус угла с направлением на центр трассы (чтобы машинка понимала, "по" или "против" часовой стрелки она едет)). Добавим к ним ещё пару входов: наши возможные действия (ускориться или замедлиться, направить это ускорение вправо или влево от текущего положения — всего по 3 варианта для величины и направления ускорения). Для каждой пары величины и направления по тому, что мы сейчас видим, и тому, что собираемся делать, предскажем, какую награду мы получим, выбрав это действие. И, очевидно, выберем то действие, которое сулит нам наибольшую награду.


# Рычаги управления

Для того чтобы показать хороший результат мы советуем вам как следует разобраться с настройками вашего болида. Во-первых, нужно как следует понять и прочувствовать, за что отвечают различные константы, поиграть с параметрами обучения нейронной сети (**learning rate**, **batch size**, количество эпох). Весьма полезным может быть изменение правил сохранения истории поощрений и наказаний, изменять распорядок обучения сети (сейчас он явно далёк от оптимального), можно менять архитектуру сети. Помните то, чему мы вас учили! Визуализируйте процесс обучения нейросети, попробуйте добавить регуляризацию =)

Обязательно стоит поэкспериментировать и с методом reward класса **SimpleCarWorld**. Очень продуктивным может быть ступенчатое обучение модели (обучили - скачали веса - обучили еще - ...). Или даже плавное обучение сложным идеям (обучили - скачали веса - изменили **reward** - обучили еще - ...). 

Список возможных изменений и экспериментов этим, разумеется, не ограничивается.

Чтобы не было недоразумений: нельзя произвольно менять базовые предположения о том, что подаётся на вход сети (из них настраивается только количество лучей лидара). Если вы собираетесь менять мир (что, конечно, похвально), то есть физику и т.д., учитывайте, что ваша нейросеть будет оцениваться на таком мире, который мы вам изначально предоставляем. Если вы не понимаете, на что влияет какой-то параметр и если этот параметр не был изначально закомментрирован как изменяемый - поймите, прежде чем его менять. Не стоит также менять активационные функции нейросети (если только ради эксперимента), поскольку в чекере активационные функции останутся прежними.
